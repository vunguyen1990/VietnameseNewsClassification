{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Get categories of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store\n",
      "dulich\n",
      "giaitri\n",
      "giaoduc\n",
      "khoahoc\n",
      "kinhdoanh\n",
      "otoxemay\n",
      "phapluat\n",
      "sohoa\n",
      "thegioi\n",
      "thethao\n",
      "thoisu\n"
     ]
    }
   ],
   "source": [
    "dir_path = os.path.join(os.getcwd(), 'vnexpress')\n",
    "categories = list()\n",
    "\n",
    "data = list()\n",
    "for directory in os.listdir(dir_path):\n",
    "#     print(directory)\n",
    "    if '.' not in directory:\n",
    "        list_file_path = os.path.join(dir_path, directory)\n",
    "        count = 0\n",
    "        for file_name in os.listdir(list_file_path):\n",
    "            data_dict = dict()\n",
    "            data_dict['category'] = directory\n",
    "            file_path = os.path.join(list_file_path, file_name)\n",
    "            file = open(file_path,'r')\n",
    "            data_dict['data'] = file.read()\n",
    "            data.append(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5)\n",
    "data_vector = vectorizer.fit_transform(data_df.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_list = data_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47487, 169465)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169465"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(data)\n",
    "data_df['category'].unique()\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "sample = random.sample(data, 10000)\n",
    "sample_df = pd.DataFrame(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_vector = vectorizer.transform(sample_df.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_vector = sample_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nhà_vua và hoàng_hậu nhật_bản thăm cố_đô huế Đông_đảo người_dân thừa_thiên_huế và người nhật sinh_sống tại huế chào_đón nhật hoàng và hoàng_hậu trong chuyến đến thăm huế nhà_vua nhật_bản_akihito và hoàng_hậu michiko chiều nay cùng đoàn công_tác đến thừa_thiên_huế bắt_đầu chuyến thăm tỉnh này trong hai ngày Đón_nhà vua và hoàng_hậu tại sân_bay quốc_tế phú_bài có ông nguyễn_ngọc_thiện bộ_trưởng bộ văn hóa thể_thao và du_lịch ông nguyễn_văn_cao chủ_tịch ubnd tỉnh thừa_thiên_huế ông lê_trường_lưu bí_thư tỉnh_uỷ thừa_thiên_huế cùng các quan_chức đại_sứ_quán nhật_bản tại việt_nam trong những ngày ở huế   nhà_vua và hoàng_hậu nhật_bản sẽ tham_quan Đại_nội huế và nghe nhã_nhạc cung_đình tham_quan khu lưu_niệm nhà yêu nước phan_bội_châu và gặp_gỡ nhân_viên tình_nguyện jica và cộng_đồng người nhật_bản tại việt_nam nhà_vua và hoàng_hậu nhật_bản đang có chuyến thăm việt_nam lần đầu_tiên từ ngày 28/2 tới 5/3 tại_hà_nội nhà_vua và hoàng_hậu gặp các thành_viên gia_đình một_số lính nhật từng ở việt_nam sau thế_chiến ii nhà_vua và hoàng_hậu nhật tới huế và sẽ rời đi bangkok thái_lan để viếng quốc_vương bhumibol_adulyadej qua_đời năm 2016 hoàng_táo - võ_thạnh\n",
      "0.32482552467\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# import tensorflow as tf\n",
    "\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "contents = df['content']\n",
    "\n",
    "print(contents[0])\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5)\n",
    "data_vector = vectorizer.fit_transform(df.content)\n",
    "\n",
    "data_contents = data_vector.toarray()\n",
    "print(max(data_contents[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
