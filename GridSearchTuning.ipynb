{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Display progress logs on stdout\n",
    "# logging.basicConfig(level=logging.INFO,\n",
    "#                     format='%(asctime)s %(levelname)s %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir_path = os.path.join(os.getcwd(), 'vnexpress')\n",
    "categories = list()\n",
    "\n",
    "data = list()\n",
    "for directory in os.listdir(dir_path):\n",
    "#     print(directory)\n",
    "    if '.' not in directory:\n",
    "        list_file_path = os.path.join(dir_path, directory)\n",
    "        count = 0\n",
    "        for file_name in os.listdir(list_file_path):\n",
    "            data_dict = dict()\n",
    "            data_dict['file_name'] = file_name\n",
    "            data_dict['category'] = directory\n",
    "            file_path = os.path.join(list_file_path, file_name)\n",
    "            file = open(file_path,'r')\n",
    "            data_dict['data'] = file.read()\n",
    "            data.append(data_dict)\n",
    "data_df = pd.DataFrame(data)\n",
    "sample_df = data_df.sample(10000)\n",
    "train, test = train_test_split(sample_df, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['tfidf', 'clf']\n",
      "parameters:\n",
      "{'clf__C': (1e-05, 1e-06),\n",
      " 'tfidf__max_df': (0.5, 0.75, 1.0),\n",
      " 'tfidf__norm': ('l1', 'l2'),\n",
      " 'tfidf__smooth_idf': (True, False)}\n",
      "done in 300.256s\n",
      "dict_keys(['tfidf', 'clf__verbose', 'tfidf__dtype', 'clf__penalty', 'tfidf__norm', 'tfidf__max_df', 'tfidf__stop_words', 'clf__class_weight', 'tfidf__encoding', 'steps', 'clf__fit_intercept', 'tfidf__min_df', 'tfidf__lowercase', 'clf', 'tfidf__tokenizer', 'tfidf__decode_error', 'tfidf__vocabulary', 'tfidf__use_idf', 'tfidf__sublinear_tf', 'tfidf__ngram_range', 'clf__solver', 'tfidf__preprocessor', 'tfidf__smooth_idf', 'tfidf__analyzer', 'clf__random_state', 'tfidf__strip_accents', 'clf__warm_start', 'tfidf__input', 'clf__dual', 'clf__intercept_scaling', 'clf__max_iter', 'tfidf__binary', 'clf__n_jobs', 'clf__C', 'clf__tol', 'tfidf__max_features', 'tfidf__token_pattern', 'clf__multi_class'])\n",
      "Best score: 0.271\n",
      "Best parameters set:\n",
      "\tclf__C: 1e-05\n",
      "\ttfidf__max_df: 0.5\n",
      "\ttfidf__norm: 'l2'\n",
      "\ttfidf__smooth_idf: True\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression()),\n",
    "])\n",
    "\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "    'tfidf__max_df': (0.5, 0.75, 1.0),\n",
    "    'tfidf__smooth_idf': (True, False),\n",
    "    'tfidf__sublinear_tf':(True, False),\n",
    "    'tfidf__binary':(True, False),\n",
    "    #'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    #'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__C': (1e4,1e5,1e6,),\n",
    "    'clf__C': (1e4,1e5,1e6,),\n",
    "#     'clf__penalty': ('l2', 'elasticnet'),\n",
    "    #'clf__n_iter': (10, 50, 80),\n",
    "}\n",
    "if __name__ == \"__main__\":\n",
    "    # multiprocessing requires the fork to happen in a __main__ protected\n",
    "    # block\n",
    "\n",
    "    # find the best parameters for both the feature extraction and the\n",
    "    # classifier\n",
    "#     grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "    grid_search = GridSearchCV(pipeline, parameters)\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    t0 = time()\n",
    "    grid_search.fit(train.data, train.category)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print(grid_search.estimator.get_params().keys())\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
