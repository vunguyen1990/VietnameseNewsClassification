{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn import linear_model\n",
    "\n",
    "import tensorflow.python.platform\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dir_path = os.path.join(os.getcwd(), 'tok_vnexpress')\n",
    "categories = list()\n",
    "\n",
    "data = list()\n",
    "\n",
    "for directory in os.listdir(dir_path):\n",
    "    i = 0\n",
    "    if '.' not in directory:\n",
    "        list_file_path = os.path.join(dir_path, directory)\n",
    "        count = 0\n",
    "        for file_name in os.listdir(list_file_path):\n",
    "            data_dict = dict()\n",
    "            data_dict['category'] = directory\n",
    "            file_path = os.path.join(list_file_path, file_name)\n",
    "            file = open(file_path,'r')\n",
    "            data_dict['data'] = file.read()\n",
    "            data.append(data_dict)\n",
    "            i = i + 1\n",
    "            if i == 100: ## For test, only get 100 document from each category\n",
    "                break\n",
    "\n",
    "data_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contents = data_df['data']\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5)\n",
    "data_vector = vectorizer.fit_transform(contents)\n",
    "\n",
    "data_contents = data_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_category_to_number(category):\n",
    "    if category == \"the-thao\":\n",
    "        return 0\n",
    "    if category == \"du-lich\":\n",
    "        return 1\n",
    "    if category == \"khoa-hoc\":\n",
    "        return 2\n",
    "    if category == \"kinh-doanh\":\n",
    "        return 3\n",
    "    if category == \"giai-tri\":\n",
    "        return 4\n",
    "    if category == \"oto-xe-may\":\n",
    "        return 5\n",
    "    if category == \"thoi-su\":\n",
    "        return 6\n",
    "    if category == \"giao-duc\":\n",
    "        return 7\n",
    "    if category == \"the-gioi\":\n",
    "        return 8\n",
    "    if category == \"phap-luat\":\n",
    "        return 9\n",
    "    if category == \"so-hoa\":\n",
    "        return 10\n",
    "    \n",
    "    raise ValueError('Category ' + category + \"not in training set\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_vectors = pd.DataFrame(data_contents)\n",
    "df_vectors['category'] = data_df['category'].apply(convert_category_to_number)\n",
    "df_vectors.to_csv(\"docs_to_vec.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.python.platform\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Global variables.\n",
    "NUM_LABELS = 11    # The number of labels.\n",
    "BATCH_SIZE = 100  # The number of training examples to use per training step.\n",
    "\n",
    "\n",
    "# Extract numpy representations of the labels and features given rows consisting of:\n",
    "#   label, feat_0, feat_1, ..., feat_n\n",
    "\n",
    "def extract_data(vector_df):\n",
    "\n",
    "    # Arrays to hold the labels and feature vectors.\n",
    "    labels = vector_df['category']\n",
    "    fvecs = vector_df.drop('category', 1)\n",
    "\n",
    "    # Convert the array of float arrays into a numpy float matrix.\n",
    "    fvecs_np = np.matrix(fvecs).astype(np.float32)\n",
    "\n",
    "    # Convert the array of int labels into a numpy array.\n",
    "    labels_np = np.array(labels).astype(dtype=np.uint8)\n",
    "    \n",
    "    # Convert the int numpy array into a one-hot matrix.\n",
    "    labels_onehot = (np.arange(NUM_LABELS) == labels_np[:, None]).astype(np.float32)\n",
    "    \n",
    "    # Return a pair of the feature matrix and the one-hot label matrix.\n",
    "\n",
    "    return fvecs_np,labels_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-59c35e6fae0f>, line 48)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-59c35e6fae0f>\"\u001b[0;36m, line \u001b[0;32m48\u001b[0m\n\u001b[0;31m    for step in range(train_size // BATCH_SIZE + 1 )\u001b[0m\n\u001b[0m                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def run():\n",
    "    train_df_vector, test_df_vector = train_test_split(df_vectors, test_size = 0.3)\n",
    "    \n",
    "    # Extract it into numpy matrices.\n",
    "    train_data,train_labels = extract_data(train_df_vector)\n",
    "    test_data, test_labels = extract_data(test_df_vector)\n",
    "\n",
    "    # Get the shape of the training data.\n",
    "    train_size,num_features = train_data.shape\n",
    "\n",
    "\n",
    "    # This is where training samples and labels are fed to the graph.\n",
    "    # These placeholder nodes will be fed a batch of training data at each\n",
    "    # training step using the {feed_dict} argument to the Run() call below.\n",
    "    x = tf.placeholder(\"float\", shape=[None, num_features])\n",
    "    y_ = tf.placeholder(\"float\", shape=[None, NUM_LABELS])\n",
    "    \n",
    "    # These are the weights that inform how much each feature contributes to\n",
    "    # the classification.\n",
    "    W = tf.Variable(tf.zeros([num_features,NUM_LABELS]))\n",
    "    b = tf.Variable(tf.zeros([NUM_LABELS]))\n",
    "    y = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "\n",
    "    # Optimization.\n",
    "    cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "\n",
    "\n",
    "    # For the test data, hold the entire dataset in one constant node.\n",
    "    test_data_node = tf.constant(test_data)\n",
    "\n",
    "    # Evaluation.\n",
    "    predicted_class = tf.argmax(y,1);\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "    # Create a local session to run this computation.\n",
    "    with tf.Session() as s:\n",
    "\n",
    "        # Run all the initializers to prepare the trainable parameters.\n",
    "        tf.initialize_all_variables().run()\n",
    "        \n",
    "        # Iterate and train.\n",
    "        print(\"train_size: \", train_size)\n",
    "        print(\"batch size: \", BATCH_SIZE)\n",
    "        print(\"train_size // BATCH_SIZE: \", train_size // BATCH_SIZE)\n",
    "        for step in range(train_size // BATCH_SIZE + 1 )\n",
    "            offset = (step * BATCH_SIZE) % train_size\n",
    "\n",
    "            # get a batch of data\n",
    "            batch_data = train_data[offset:(offset + BATCH_SIZE), :]\n",
    "            batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "\n",
    "            # feed data into the model\n",
    "            train_step.run(feed_dict={x: batch_data, y_: batch_labels})\n",
    "            \n",
    "            \n",
    "        print (\"Accuracy:\", accuracy.eval(feed_dict={x: test_data, y_: test_labels}))\n",
    "        eval_fun = lambda X: predicted_class.eval(feed_dict={x:X}); \n",
    "        \n",
    "        predict_classes = predicted_class.eval(feed_dict={x:test_data})\n",
    "        actual_classes = test_labels.argmax(axis=1)\n",
    "        return predict_classes, actual_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_classes, actual_classes = run()\n",
    "pd.crosstab(actual_classes, predict_classes, margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision_recall_fscore_support(actual_classes, predict_classes, average='macro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
